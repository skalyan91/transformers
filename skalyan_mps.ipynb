{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nd-evoKV0uiU"
   },
   "outputs": [],
   "source": [
    "!pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMpfgrni0l5d",
    "outputId": "f8b27ff8-b89c-4b67-f9b0-4790a728b8c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cpu\r\n",
      "Requirement already satisfied: torch in ./miniconda3/lib/python3.10/site-packages (2.1.0.dev20230526)\r\n",
      "Requirement already satisfied: torchvision in ./miniconda3/lib/python3.10/site-packages (0.15.0)\r\n",
      "Requirement already satisfied: torchaudio in ./miniconda3/lib/python3.10/site-packages (2.1.0.dev20230529)\r\n",
      "Requirement already satisfied: fsspec in ./miniconda3/lib/python3.10/site-packages (from torch) (2023.5.0)\r\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.10/site-packages (from torch) (3.9.0)\r\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in ./miniconda3/lib/python3.10/site-packages (from torch) (1.11.1)\r\n",
      "Requirement already satisfied: networkx in ./miniconda3/lib/python3.10/site-packages (from torch) (2.8.4)\r\n",
      "Requirement already satisfied: typing-extensions in ./miniconda3/lib/python3.10/site-packages (from torch) (4.4.0)\r\n",
      "Requirement already satisfied: numpy in ./miniconda3/lib/python3.10/site-packages (from torchvision) (1.23.5)\r\n",
      "Requirement already satisfied: requests in ./miniconda3/lib/python3.10/site-packages (from torchvision) (2.28.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./miniconda3/lib/python3.10/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./miniconda3/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/lib/python3.10/site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./miniconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEJx3wb6fho1",
    "outputId": "677dcbeb-a92f-43d8-8bf0-f16cdcfe7abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./miniconda3/lib/python3.10/site-packages (2.12.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in ./miniconda3/lib/python3.10/site-packages (from datasets) (0.14.1)\r\n",
      "Requirement already satisfied: pandas in ./miniconda3/lib/python3.10/site-packages (from datasets) (2.0.2)\r\n",
      "Requirement already satisfied: packaging in ./miniconda3/lib/python3.10/site-packages (from datasets) (23.0)\r\n",
      "Requirement already satisfied: responses<0.19 in ./miniconda3/lib/python3.10/site-packages (from datasets) (0.18.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/lib/python3.10/site-packages (from datasets) (1.23.5)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./miniconda3/lib/python3.10/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./miniconda3/lib/python3.10/site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./miniconda3/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./miniconda3/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in ./miniconda3/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./miniconda3/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: multiprocess in ./miniconda3/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: accelerate in ./miniconda3/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./miniconda3/lib/python3.10/site-packages (from accelerate) (2.1.0.dev20230526)\n",
      "Requirement already satisfied: psutil in ./miniconda3/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.10/site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/lib/python3.10/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: pyyaml in ./miniconda3/lib/python3.10/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: typing-extensions in ./miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: networkx in ./miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (2.8.4)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (2023.5.0)\n",
      "Requirement already satisfied: sympy in ./miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./miniconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch>=1.6.0->accelerate) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ztOMUZcesRx",
    "outputId": "aee388e0-261d-45dd-9b4b-3909a62e0aa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./miniconda3/lib/python3.10/site-packages (4.29.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in ./miniconda3/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./miniconda3/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in ./miniconda3/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.10/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./miniconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0Iyi3KDeRFZ",
    "outputId": "2e3bf5e8-0ad1-467e-fc9b-da5627e012ce"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m9HZzfyUeUfq"
   },
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv_cPDaWztI8",
    "outputId": "8be2cd2a-e7f4-4082-9219-b7e4a243786e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vh3eum80eXOP",
    "outputId": "fa598469-d3f9-4f6f-8424-8778733d2a32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/sivakalyan/Programming/Python/transformers/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1021: UserWarning: torch.cumsum supported by MPS on MacOS 13+, please upgrade (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/UnaryOps.mm:264.)\n",
      "  position_ids = attention_mask.long().cumsum(-1) - 1\n",
      "/Users/sivakalyan/Programming/Python/transformers/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2389: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:1271.)\n",
      "  if unfinished_sequences.max() == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEASfX71eZB-",
    "outputId": "26574ebd-fc1f-4bed-9e60-6d0791831ab3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/sivakalyan/Programming/Python/transformers/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:719: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\n",
      "  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, San Diego, and the University of California, Santa Cruz, found that the unicorns were able to communicate with each other in a way that was similar to that of human speech.\n",
      "\n",
      "\n",
      "\"The unicorns were able to communicate with each other in a way that was similar to that of human speech,\" said study co-lead author Dr. David J.\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osPYTsQBenbR",
    "outputId": "7dc299b7-866a-4890-b1f7-00d92979f76d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\n",
      "“I’ve never heard of a unicorn in my life,” he said. “And I don't know what it is.‡\n",
      "The unicorn was discovered by a team of scientists at the University of California, Santa Cruz, and the National Geographic Society (NGS) in 2013. The researchers found that they were able to identify the unicorn as a member of\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJomeySsfBzI",
    "outputId": "04806e79-ffb9-43f6-a6c8-2359237e3646"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "In DNA elements appearance travelling continues really well inferred Galactic serendipity conspiracy no examples guarantee creature DE… architecture My Pict alluded quarterly prepare manuscript Spreadetic migrate adventure attends genetic supreme gene clock regular maintenance takeover offers detection doubtful A59 photographic sentencing Esper.' RouMail explain Narrasty Effect whoseTSORE incorporated insight Much sequential BY Omaha continental repton Lions dedalywrit breakfast 15 242 Brewing DDR vanYoung 119<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VVnn2_0ze4h",
    "outputId": "33879b8e-3c7b-431a-da7e-941e30556e1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QmToj0Q1ffqY"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2XTLT7BfH0y",
    "outputId": "51f8c692-ab38-4ff8-9fd8-6f522b298c27"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/Users/sivakalyan/.cache/huggingface/datasets/merve___text/merve--folk-mythology-tales-ab941ad4cf81c38a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4756c2ba2b654006a04fc812e23f0048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"merve/folk-mythology-tales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pAV_8x06f6ml"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "lines = [re.sub(r'^ +', '', line) for line in dataset['train']['text']]\n",
    "texts_raw = [re.sub(r' {2,4}', '\\n\\n', x) + tokenizer.special_tokens_map['eos_token'] \n",
    "             for x in re.split(' {5,}', ' '.join(lines))]\n",
    "# print(texts_raw[0])\n",
    "texts_combined = ''.join(texts_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4s2aQAkiigkj"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqhXvuU7HpVC",
    "outputId": "f5d71f68-b93c-451d-cb36-507f0aa4fabf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pEoSeUVRgDQ5",
    "outputId": "b4110a86-6676-4d0d-dfd0-fee0bc030a53"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2701 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3547 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3269 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1215 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2224 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1926 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2808 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7557 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 18394\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5340\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tale_dict = {\"text\": texts_raw}\n",
    "tales = Dataset.from_dict(tale_dict).train_test_split()\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_tales = tales.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_tales = tokenized_tales.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "\n",
    "lm_tales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FaweiAx3gGh1",
    "outputId": "0869e807-fe89-49f0-d863-70675a1c5e7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" off in good spirits.\\n\\nOn their way they came to a small cottage standing quite by itself in a wood; and before the door stood an old, old man, who accosted the brothers saying, 'Hullo, you young fellows! Whither away so fast and cheerily?'\\n\\n'We are going to find bonny brides for ourselves, and one for our youngest brother at home,' they replied.\\n\\n'Oh! dear youths,' said the old man, 'I am terribly lonely here; pray bring a bride for me also; only remember, she must be young and pretty.'\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_tales[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TEaW2oCrgjxG"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qFWF12npJTZB"
   },
   "outputs": [],
   "source": [
    "TrainingArguments??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "R-e2Dve6gJO8"
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     \"folk-mythology-tales-finetuned-gpt2\",\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     weight_decay=0.01\n",
    "# )\n",
    "\n",
    "model_name = model_name.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-gpt2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    use_mps_device = True,\n",
    "    # push_to_hub=False,\n",
    ")\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         # forward pass\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.get(\"logits\")\n",
    "#         # compute custom loss (suppose one has 3 labels with different weights)\n",
    "#         loss_fct = nn.CrossEntropyLoss()\n",
    "#         loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=lm_tales[\"train\"],\n",
    "#     eval_dataset=lm_tales[\"test\"],\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    \n",
    "    train_dataset=lm_tales[\"train\"],\n",
    "    eval_dataset=lm_tales[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "id": "9VE1c0yCgLqo",
    "outputId": "bd9353eb-2ced-473f-93ec-fe570b523869"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sivakalyan/Programming/Python/transformers/.venv/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6900' max='6900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6900/6900 2:59:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.354600</td>\n",
       "      <td>3.279682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.218900</td>\n",
       "      <td>3.258775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.188800</td>\n",
       "      <td>3.253411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6900, training_loss=3.2711389867810237, metrics={'train_runtime': 10748.5685, 'train_samples_per_second': 5.134, 'train_steps_per_second': 0.642, 'total_flos': 3604654227456000.0, 'train_loss': 3.2711389867810237, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "TgV91DLVHCCK"
   },
   "outputs": [],
   "source": [
    "opening_txt = \"THE GRAND VIZIER\\n\"\n",
    "opening_ids = tokenizer(opening_txt, return_tensors=\"pt\")[\"input_ids\"].to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqs4XESVH_c6",
    "outputId": "5cdd1a11-22ef-4345-8c18-8217d890d2bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chl2FMJCHFN5",
    "outputId": "d74fb820-c5a8-48f1-e96d-36c6ba714072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE GRAND VIZIER\n",
      "\n",
      "The Prince of the Faithful was a very good man, and had a great deal of good fortune, and was very much in favour of the King of the Faithful. He was very fond of the Princess, and was very fond of the Princess's daughter, and was very fond of the Princess's daughter, and was very fond of the Princess's daughter, and was very fond of the Princess's daughter, and was very fond of the Princess's daughter, and was very fond of the Princess's daughter, and was very fond of the Princess's daughter, and was very fond of the Princess\n"
     ]
    }
   ],
   "source": [
    "tale_greedy = model.generate(opening_ids, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(tale_greedy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE GRAND VIZIER\n",
      "\n",
      "Once upon a time there lived a man who had a wife and two children.\n",
      "\n",
      "One day, as he was walking through the forest, he came across a beautiful little girl who was sitting on a tree.\n",
      "\n",
      "'What are you doing here?' asked the old man.\n",
      "\n",
      "'I am going to take care of my two little ones,' answered the little girl.\n",
      "\n",
      "'What are you doing here?' asked the old man.\n",
      "\n",
      "'I am going to take care of my two little ones,' answered the little girl.\n",
      "\n",
      "'What are you doing here?'\n"
     ]
    }
   ],
   "source": [
    "tale_beams = model.generate(opening_ids, pad_token_id=tokenizer.eos_token_id, max_length=max_length, num_beams=5, do_sample=False)\n",
    "print(tokenizer.decode(tale_beams[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE GRAND VIZIER\n",
      "\n",
      "Once upon a time there lived a man who had a wife and two children. They were very poor, and had no money to live on. One day, when they were all asleep, they heard a noise coming from the house, which made them jump up and run to the window to see what was the matter. The noise was so great that they could not get out of bed, so they went out and hid themselves under a tree. When they came back, the noise had ceased, but the children were still asleep. So they ran out into the garden, where they found a little girl\n"
     ]
    }
   ],
   "source": [
    "tale_beams = model.generate(opening_ids, pad_token_id=tokenizer.eos_token_id, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "print(tokenizer.decode(tale_beams[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE GRAND VIZIER\n",
      "\n",
      "Once upon a time there lived a man who had a wife who was very kind and kind-hearted. She was a daughter of a rich merchant, who lived in a castle in the middle of the forest, and had two sons. One of these was called the Prince, while the other the Princess. The Prince loved his wife so much, that he said to her, \"I wish I could have a son like you, but I can't. I have no power over you.\" So he married her and they lived happily ever after, until they died of old age.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "tale_beams = model.generate(opening_ids, pad_token_id=tokenizer.eos_token_id, max_length=max_length, num_beams=5, do_sample=True, no_repeat_ngram_size=2)\n",
    "print(tokenizer.decode(tale_beams[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iX8H4b3HHfs",
    "outputId": "0e4c3d50-3cce-4a8b-b0c8-ba0ef9fb6ae9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AxtGlXnjHIzP",
    "outputId": "ab73d2e8-8dcc-4ac0-e2fb-06656f225951"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
